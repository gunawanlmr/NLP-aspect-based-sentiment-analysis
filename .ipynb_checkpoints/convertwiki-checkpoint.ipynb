{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Converting Wikipedia XML Dump Files to Clean Text\n",
    "\n",
    "This notebook is about downloading wikipedia dump file, process the articles, cleaning them, and save it for later use.\n",
    "\n",
    "All the functions in this notebook will be running in \"lazy\" behavior.\n",
    "\n",
    "We don't want to read the whole file, process the whole file, as it would be memory consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the imports.\n",
    "- bz2 - for extracting the downloaded file\n",
    "- json - the \"clean text\" data we're saving is a bytes of json with format {\"text\": \"the content of articles\"}\n",
    "- os - for file stat, os.path functionalities\n",
    "- re - for cleaning texts\n",
    "- ElementTree - for parsing json\n",
    "- six.moves.urllib - used to download the wikipedia file\n",
    "- time - for tracking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from six.moves import urllib\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the wikipedia file.\n",
    "\n",
    "You may change the url and filename, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified idwiki-20170620-pages-articles.xml.bz2\n",
      "The file is in: idwiki-20170620-pages-articles.xml.bz2\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(url, filename, expected_bytes=None):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"getting from: {}\".format(url))\n",
    "        filename, _ = urllib.request.urlretrieve(url, filename)\n",
    "    if expected_bytes:\n",
    "        statinfo = os.stat(filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', filename)\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception(\n",
    "                'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "\n",
    "bz2file = maybe_download(\"https://dumps.wikimedia.org/idwiki/20170620/idwiki-20170620-pages-articles.xml.bz2\", \"idwiki-20170620-pages-articles.xml.bz2\", 409688848)\n",
    "print(\"The file is in:\", bz2file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying the file, firstly, extract the .bz2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idwiki-20170620-pages-articles.xml alread existed\n",
      "file size: 1.949 GB\n"
     ]
    }
   ],
   "source": [
    "def extract_bz2(filename):\n",
    "    fname, ext = os.path.splitext(filename)\n",
    "    if ext != \".bz2\":\n",
    "        raise ValueError(\"filename specified is not a .bz2\")\n",
    "    if os.path.exists(fname):\n",
    "        print(fname, \"alread existed\")\n",
    "        return fname\n",
    "\n",
    "    with open(fname, \"wb\") as f, bz2.BZ2File(filename, \"rb\") as bf:\n",
    "        for data in iter(lambda : bf.read(100*1024), b''):\n",
    "            _ = f.write(data)\n",
    "    return fname\n",
    "\n",
    "\n",
    "xmlfile = extract_bz2(bz2file)\n",
    "statinfo = os.stat(xmlfile)\n",
    "print(\"file size: {:.3f} GB\".format(statinfo.st_size / (1024*1024*1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting .bz2 file, we now get the .xml file.\n",
    "\n",
    "It's time to parse the XML file to get pages of article.\n",
    "\n",
    "This functionality is similar to:\n",
    "\n",
    "```\n",
    "def read_xml(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    pages = root.findall('export-0.1:page', ns)\n",
    "    return pages\n",
    "```\n",
    "\n",
    "However, this code consumes too much memory, that a 8GB memory instance still experiencing a MemoryError.\n",
    "\n",
    "Hence, we use the `iterparse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = {'export-0.1': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "tags_to_skip = [\"siteinfo\"]\n",
    "\n",
    "\n",
    "def parse_wiki_xml(filename):\n",
    "    skipping = \"\"\n",
    "    in_page = False\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\", \"end\",)):\n",
    "        if event == \"start\":\n",
    "            for tag in tags_to_skip:\n",
    "                if tag in elem.tag:\n",
    "                    print(\"removing elem siteinfo\")\n",
    "                    skipping = tag\n",
    "                    elem.clear()\n",
    "                    break\n",
    "            if in_page:\n",
    "                continue\n",
    "            if \"page\" in elem.tag:\n",
    "                in_page = True\n",
    "#             if not skipping and \"page\" not in elem.tag:\n",
    "#                 print(\"start event for tag:\", elem.tag)\n",
    "        elif event == \"end\":\n",
    "            if skipping:\n",
    "                if skipping in elem.tag:\n",
    "                    elem.clear()\n",
    "                    skipping = \"\"\n",
    "            else:\n",
    "                if \"page\" in elem.tag:\n",
    "                    yield elem\n",
    "                    elem.clear()\n",
    "                    in_page = False\n",
    "\n",
    "\n",
    "pages = parse_wiki_xml('/media/dispsiau-2013/FE6CC69D6CC65057/Users/Dispsiau 2013/Documents/Fasilkom015/idwiki-20180501-pages-articles.xml/idwiki-20180501-pages-articles.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XML file needs some cleaning. Hence we create this function.\n",
    "\n",
    "This function, `process_text`, is similar to the Perl code available in http://mattmahoney.net/dc/textdata (see Appendix A for `wikifil.pl`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Remove any text not normally visible\n",
    "    text = re.sub(r\"<.*>\", \"\", text)  # remove xml tags\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)  # decode URL encoded chars\n",
    "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"<ref[^<]*</ref>\", \"\", text)  # remove references <ref...> ... </ref>\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)  # remove xhtml tags\n",
    "    text = re.sub(r\"\\[http:[^] ]*\", \"[\", text)  # remove normal url, preserve visible text\n",
    "    text = re.sub(r\"\\|thumb\", \"\", text)  # remove images links, preserve caption\n",
    "    text = re.sub(r\"\\|left\", \"\", text)\n",
    "    text = re.sub(r\"\\|right\", \"\", text)\n",
    "    text = re.sub(r\"\\|\\d+px\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\[image:[^\\[\\]]*\\|\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\", r\"\\1\", text, flags=re.I)  # show categories without markup\n",
    "    text = re.sub(r\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\", \"\", text)  # remove links to other languages\n",
    "    text = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", text)  # remove wiki url, preserve visible text\n",
    "    text = re.sub(r\"{{[^}]*}}\", \"\", text)  # remove {{icons}} and {tables}\n",
    "    text = re.sub(r\"{[^}]*}\", \"\", text)\n",
    "    text = re.sub(r\"\\[\", \"\", text)  # remove [ and ]\n",
    "    text = re.sub(r\"\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)  # remove ( and )\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"&[^;]*;\", \" \", text)  # remove URL encoded chars\n",
    "    text = re.sub(r\"\\\"\", \"\", text)  # remove ' and \"\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"_\", \"\", text)  # remove _\n",
    "    text = re.sub(r\"\\W+\", \" \", text)\n",
    "    text = re.sub(r\"0\", \" nol \", text)\n",
    "    text = re.sub(r\"1\", \" satu \", text)\n",
    "    text = re.sub(r\"2\", \" dua \", text)\n",
    "    text = re.sub(r\"3\", \" tiga \", text)\n",
    "    text = re.sub(r\"4\", \" empat \", text)\n",
    "    text = re.sub(r\"5\", \" lima \", text)\n",
    "    text = re.sub(r\"6\", \" enam \", text)\n",
    "    text = re.sub(r\"7\", \" tujuh \", text)\n",
    "    text = re.sub(r\"8\", \" delapan \", text)\n",
    "    text = re.sub(r\"9\", \" sembilan \", text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a function to clean text. It's time to process the wikipedia pages and convert them into clean texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_text(pages):\n",
    "    for i, page in enumerate(pages):\n",
    "        if i % 20000 == 19999:\n",
    "            print(\"Read {}k articles. Elapsed time: {:.3f}s\".format(int((i+1)/1000), time() - t0))\n",
    "\n",
    "        title = page.find('export-0.1:title', ns).text.lower()\n",
    "        if title.startswith(\"wikipedia:catatan commons\"):\n",
    "            page.clear()\n",
    "            continue\n",
    "        del title\n",
    "\n",
    "        text = page.find('export-0.1:revision', ns).find('export-0.1:text', ns).text\n",
    "        if not text:\n",
    "            page.clear()\n",
    "            continue\n",
    "\n",
    "        text = process_text(text)\n",
    "\n",
    "        words = text.split()\n",
    "        del text\n",
    "        total_words = len(words)\n",
    "        total_long_words = len([w for w in words if len(w) > 3])\n",
    "        if total_long_words < 15:\n",
    "            page.clear()\n",
    "            continue\n",
    "        yield \" \".join(words)\n",
    "\n",
    "\n",
    "texts = convert_to_text(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start writing wikipedia texts to idwiki-20170620-pages-articles.text\n",
      "removing elem siteinfo\n",
      "Done writing 10k pages. Elapsed time: 18.612s\n",
      "Read 20k articles. Elapsed time: 22.436s\n",
      "Done writing 20k pages. Elapsed time: 35.760s\n",
      "Read 40k articles. Elapsed time: 38.214s\n",
      "Done writing 30k pages. Elapsed time: 48.134s\n",
      "Read 60k articles. Elapsed time: 48.445s\n",
      "Read 80k articles. Elapsed time: 57.816s\n",
      "Done writing 40k pages. Elapsed time: 61.211s\n",
      "Read 100k articles. Elapsed time: 67.440s\n",
      "Done writing 50k pages. Elapsed time: 73.638s\n",
      "Read 120k articles. Elapsed time: 79.241s\n",
      "Done writing 60k pages. Elapsed time: 87.741s\n",
      "Read 140k articles. Elapsed time: 90.314s\n",
      "Done writing 70k pages. Elapsed time: 97.043s\n",
      "Read 160k articles. Elapsed time: 100.681s\n",
      "Done writing 80k pages. Elapsed time: 109.359s\n",
      "Read 180k articles. Elapsed time: 112.199s\n",
      "Done writing 90k pages. Elapsed time: 121.453s\n",
      "Read 200k articles. Elapsed time: 123.773s\n",
      "Read 220k articles. Elapsed time: 133.969s\n",
      "Done writing 100k pages. Elapsed time: 135.186s\n",
      "Read 240k articles. Elapsed time: 141.663s\n",
      "Read 260k articles. Elapsed time: 148.118s\n",
      "Done writing 110k pages. Elapsed time: 156.116s\n",
      "Read 280k articles. Elapsed time: 156.244s\n",
      "Read 300k articles. Elapsed time: 163.689s\n",
      "Read 320k articles. Elapsed time: 169.501s\n",
      "Done writing 120k pages. Elapsed time: 177.318s\n",
      "Read 340k articles. Elapsed time: 178.662s\n",
      "Done writing 130k pages. Elapsed time: 185.259s\n",
      "Read 360k articles. Elapsed time: 186.849s\n",
      "Done writing 140k pages. Elapsed time: 195.016s\n",
      "Read 380k articles. Elapsed time: 195.937s\n",
      "Read 400k articles. Elapsed time: 206.026s\n",
      "Done writing 150k pages. Elapsed time: 206.282s\n",
      "Read 420k articles. Elapsed time: 214.411s\n",
      "Done writing 160k pages. Elapsed time: 218.285s\n",
      "Read 440k articles. Elapsed time: 224.253s\n",
      "Done writing 170k pages. Elapsed time: 233.869s\n",
      "Read 460k articles. Elapsed time: 238.821s\n",
      "Read 480k articles. Elapsed time: 244.050s\n",
      "Done writing 180k pages. Elapsed time: 254.986s\n",
      "Read 500k articles. Elapsed time: 256.271s\n",
      "Read 520k articles. Elapsed time: 263.992s\n",
      "Done writing 190k pages. Elapsed time: 268.111s\n",
      "Read 540k articles. Elapsed time: 276.935s\n",
      "Done writing 200k pages. Elapsed time: 281.689s\n",
      "Read 560k articles. Elapsed time: 287.825s\n",
      "Done writing 210k pages. Elapsed time: 288.660s\n",
      "Done writing 220k pages. Elapsed time: 293.066s\n",
      "Read 580k articles. Elapsed time: 296.643s\n",
      "Done writing 230k pages. Elapsed time: 297.476s\n",
      "Done writing 240k pages. Elapsed time: 301.728s\n",
      "Read 600k articles. Elapsed time: 305.336s\n",
      "Done writing 250k pages. Elapsed time: 306.131s\n",
      "Done writing 260k pages. Elapsed time: 310.318s\n",
      "Read 620k articles. Elapsed time: 313.616s\n",
      "Done writing 270k pages. Elapsed time: 314.426s\n",
      "Done writing 280k pages. Elapsed time: 318.564s\n",
      "Read 640k articles. Elapsed time: 321.647s\n",
      "Done writing 290k pages. Elapsed time: 322.482s\n",
      "Done writing 300k pages. Elapsed time: 326.368s\n",
      "Read 660k articles. Elapsed time: 329.411s\n",
      "Done writing 310k pages. Elapsed time: 331.095s\n",
      "Read 680k articles. Elapsed time: 339.747s\n",
      "Done writing 320k pages. Elapsed time: 343.748s\n",
      "Read 700k articles. Elapsed time: 349.754s\n",
      "Read 720k articles. Elapsed time: 355.669s\n",
      "Done writing 330k pages. Elapsed time: 355.898s\n",
      "Read 740k articles. Elapsed time: 362.071s\n",
      "Done writing 340k pages. Elapsed time: 367.301s\n",
      "Read 760k articles. Elapsed time: 372.134s\n",
      "Done writing 350k pages. Elapsed time: 377.703s\n",
      "Read 780k articles. Elapsed time: 382.497s\n",
      "Done writing 360k pages. Elapsed time: 388.462s\n",
      "Read 800k articles. Elapsed time: 393.961s\n",
      "Done writing 370k pages. Elapsed time: 400.545s\n",
      "Read 820k articles. Elapsed time: 407.228s\n",
      "Done writing 380k pages. Elapsed time: 412.505s\n",
      "Read 840k articles. Elapsed time: 418.176s\n",
      "Done writing 390k pages. Elapsed time: 423.400s\n",
      "Read 860k articles. Elapsed time: 428.432s\n",
      "Read 880k articles. Elapsed time: 435.432s\n",
      "Read 900k articles. Elapsed time: 440.420s\n",
      "Read 920k articles. Elapsed time: 445.203s\n",
      "Read 940k articles. Elapsed time: 449.989s\n",
      "Read 960k articles. Elapsed time: 455.084s\n",
      "Read 980k articles. Elapsed time: 461.030s\n",
      "Read 1000k articles. Elapsed time: 466.796s\n",
      "Read 1020k articles. Elapsed time: 473.302s\n",
      "Done writing 400k pages. Elapsed time: 473.359s\n",
      "Done writing 410k pages. Elapsed time: 483.089s\n",
      "Done writing wikipedia texts in 483.729s\n"
     ]
    }
   ],
   "source": [
    "text_filename = os.path.splitext(xmlfile)[0]+\".text\"\n",
    "print(\"start writing wikipedia texts to {}\".format(text_filename))\n",
    "t0 = time()\n",
    "\n",
    "with open(text_filename, \"wb\") as f:\n",
    "    for i, text in enumerate(texts):\n",
    "        f.write((json.dumps({\"text\": text}) + os.linesep).encode(\"utf-8\"))\n",
    "        if i % 10000 == 9999:\n",
    "            print(\"Done writing {}k pages. Elapsed time: {:.3f}s\".format(int((i+1)/1000), time() - t0))\n",
    "print(\"Done writing wikipedia texts in {:.3f}s\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading the file, use this function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total line: 410495\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def read_text_data(filename=\"idwiki-20170620-pages-articles.text\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            row = json.loads(line.decode(\"utf-8\"))\n",
    "            text = row[\"text\"]\n",
    "            yield text\n",
    "            i += 1\n",
    "        print(\"total line: {}\".format(i))\n",
    "        \n",
    "for item in read_text_data():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
